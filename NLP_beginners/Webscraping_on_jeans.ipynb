{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from selenium) (1.25.8)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path =r'C:\\Users\\parul\\AppData\\Local\\Temp\\Temp1_chromedriver_win32.zip\\chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(executable_path=driver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_page=driver.get('https://www.flipkart.com/clothing-and-accessories/bottomwear/jeans/men-jeans/pr?sid=clo,vua,k58,i51&otracker=categorytree&otracker=nmenu_sub_Men_0_Jeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname=\"Men Ethnics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(DIRNAME):\n",
    "    current_path=os.getcwd()\n",
    "    path=os.path.join(current_path,DIRNAME)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page=1\n",
    "total_pages=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_image_url(driver):\n",
    "    images = driver.find_elements_by_xpath(\"//img[@class='_3togXc']\")\n",
    "    product_data={}\n",
    "    product_data['image_urls']=[]\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['image_urls'].append(source)\n",
    "    print(\"Returning scraped data\")\n",
    "    return product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(data,DIRNAME,page):\n",
    "    for index ,link in enumerate(data['image_urls']):\n",
    "        print(\"Downloading {0} of {1} images\".format(index+1, len(data['image_urls'])))\n",
    "        response=requests.get(link)\n",
    "        with open('{0}/img_{1}{2}.jpeg'.format(DIRNAME,page,index),\"wb\") as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data,filename):\n",
    "    df=pd.DataFrame(data)\n",
    "    df.to_csv(filename,mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning scraped data\n",
      "Scraping Page 1 of 2 pages\n",
      "The current page scraped is 1\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page 1 done\n",
      "Moving to the next page\n",
      "The new page is ()\n",
      "Returning scraped data\n",
      "Scraping Page 2 of 2 pages\n",
      "The current page scraped is 1\n",
      "Downloading 1 of 40 images\n",
      "Downloading 2 of 40 images\n",
      "Downloading 3 of 40 images\n",
      "Downloading 4 of 40 images\n",
      "Downloading 5 of 40 images\n",
      "Downloading 6 of 40 images\n",
      "Downloading 7 of 40 images\n",
      "Downloading 8 of 40 images\n",
      "Downloading 9 of 40 images\n",
      "Downloading 10 of 40 images\n",
      "Downloading 11 of 40 images\n",
      "Downloading 12 of 40 images\n",
      "Downloading 13 of 40 images\n",
      "Downloading 14 of 40 images\n",
      "Downloading 15 of 40 images\n",
      "Downloading 16 of 40 images\n",
      "Downloading 17 of 40 images\n",
      "Downloading 18 of 40 images\n",
      "Downloading 19 of 40 images\n",
      "Downloading 20 of 40 images\n",
      "Downloading 21 of 40 images\n",
      "Downloading 22 of 40 images\n",
      "Downloading 23 of 40 images\n",
      "Downloading 24 of 40 images\n",
      "Downloading 25 of 40 images\n",
      "Downloading 26 of 40 images\n",
      "Downloading 27 of 40 images\n",
      "Downloading 28 of 40 images\n",
      "Downloading 29 of 40 images\n",
      "Downloading 30 of 40 images\n",
      "Downloading 31 of 40 images\n",
      "Downloading 32 of 40 images\n",
      "Downloading 33 of 40 images\n",
      "Downloading 34 of 40 images\n",
      "Downloading 35 of 40 images\n",
      "Downloading 36 of 40 images\n",
      "Downloading 37 of 40 images\n",
      "Downloading 38 of 40 images\n",
      "Downloading 39 of 40 images\n",
      "Downloading 40 of 40 images\n",
      "Scraping of page 2 done\n",
      "Moving to the next page\n",
      "The new page is ()\n"
     ]
    }
   ],
   "source": [
    "for page in range(start_page,total_pages+1):\n",
    "    try:\n",
    "        product_details=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping Page {0} of {1} pages\".format(page,total_pages))\n",
    "        \n",
    "        page_value=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"The current page scraped is {}\".format(page_value))\n",
    "        \n",
    "        save_images(data=product_details,DIRNAME=dirname,page=page)\n",
    "        print(\"Scraping of page {0} done\".format(page))\n",
    "        \n",
    "        save_data_to_csv(data=product_details,filename='men_ethnic.csv')\n",
    "        \n",
    "        print(\"Moving to the next page\")\n",
    "        button_type=driver.find_element_by_xpath(\"//div[@class='_2zg3yZ']//a[@class='_3fVaIS']//span\").get_attribute('innerHTML')\n",
    "        \n",
    "        if button_type=='Next':\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS']\").click()\n",
    "        else:\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS'][2]\").click()\n",
    "        \n",
    "        new_page = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"The new page is ()\".format(new_page))\n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print(\"We are facing an exception\")\n",
    "        \n",
    "        exp_page = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"The value at the time of exception is {}\".format(exp_page))\n",
    "        \n",
    "        value = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        \n",
    "        product_details=scrap_img_url(driver=driver)\n",
    "        print(\"Scraping Page {0} of {1} pages\".format(page,total_pages))\n",
    "        \n",
    "        page_value=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"The current page scraped is {}\".format(page_value))\n",
    "        \n",
    "        save_images(data=product_details,DIRNAME=dirname,page=page)\n",
    "        print(\"Scraping of page {0} done\".format(page))\n",
    "        \n",
    "        save_data_to_csv(data=product_details,filename='men_ethnic.csv')\n",
    "        \n",
    "        print(\"Moving to the next page\")\n",
    "        button_type=driver.find_element_by_xpath(\"//div[@class='_2zg3yZ']//a[@class='_3fVaIS']//span\").get_attribute('innerHTML')\n",
    "        \n",
    "        if button_type=='Next':\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS']\").click()\n",
    "        else:\n",
    "            driver.find_element_by_xpath(\"//a[@class='_3fVaIS'][2]\").click()\n",
    "        \n",
    "        new_page = driver.find_element_by_xpath(\"//a[@class='_2Xp0TH fyt9Eu']\").text\n",
    "        print(\"The new page is ()\".format(new_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
