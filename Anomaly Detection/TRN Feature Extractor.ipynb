{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amUilBIwX40d"
   },
   "source": [
    "#### Defining Function to extract frame for each video, simultaneously extract featured from the respective frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " pip install opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9XDKsrUfWig5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BtQ6wp4tXoxD"
   },
   "outputs": [],
   "source": [
    "SAVING_FRAMES_PER_SECOND = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fIcXLx6vXq3H"
   },
   "outputs": [],
   "source": [
    "def format_timedelta(td):\n",
    "    \"\"\"Utility function to format timedelta objects in a cool way (e.g 00:00:20.05) \n",
    "    omitting microseconds and retaining milliseconds\"\"\"\n",
    "    result = str(td)\n",
    "    try:\n",
    "        result, ms = result.split(\".\")\n",
    "    except ValueError:\n",
    "        return (result + \".00\").replace(\":\", \"-\")\n",
    "    ms = int(ms)\n",
    "    ms = round(ms / 1e4)\n",
    "    return f\"{result}.{ms:02}\".replace(\":\", \"-\")\n",
    "\n",
    "\n",
    "def get_saving_frames_durations(cap, saving_fps):\n",
    "    \"\"\"A function that returns the list of durations where to save the frames\"\"\"\n",
    "    s = []\n",
    "    # get the clip duration by dividing number of frames by the number of frames per second\n",
    "    clip_duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS)\n",
    "    # use np.arange() to make floating-point steps\n",
    "    for i in np.arange(0, clip_duration, 1 / saving_fps):\n",
    "        s.append(i)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class RelationModuleMultiScale(torch.nn.Module):\n",
    "    # Temporal Relation module in multiply scale, suming over [2-frame relation, 3-frame relation, ..., n-frame relation]\n",
    "\n",
    "    def __init__(self, img_feature_dim, num_frames, num_class):\n",
    "        super(RelationModuleMultiScale, self).__init__()\n",
    "        self.subsample_num = 4 # how many relations selected to sum up\n",
    "        self.img_feature_dim = img_feature_dim\n",
    "        self.scales = [i for i in range(num_frames, 1, -1)] # generate the multiple frame relations\n",
    "\n",
    "        self.relations_scales = []\n",
    "        self.subsample_scales = []\n",
    "        \n",
    "        for scale in self.scales:\n",
    "            relations_scale = self.return_relationset(num_frames, scale)\n",
    "            self.relations_scales.append(relations_scale)\n",
    "            self.subsample_scales.append(min(self.subsample_num, len(relations_scale))) # how many samples of relation to select in each forward pass\n",
    "\n",
    "        self.num_class = num_class\n",
    "        self.num_frames = num_frames\n",
    "        num_bottleneck = 256\n",
    "        self.fc_fusion_scales = nn.ModuleList() # high-tech modulelist\n",
    "        for i in range(len(self.scales)):\n",
    "            scale = self.scales[i]\n",
    "            fc_fusion = nn.Sequential(\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(scale * self.img_feature_dim, num_bottleneck), # mutiplies the scale with image dimension to reduce/magnify the image respectively \n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(num_bottleneck, self.num_class),\n",
    "                        )  # Simple neural network to extract features at different scales of the extracted frames\n",
    "\n",
    "            self.fc_fusion_scales += [fc_fusion]\n",
    "\n",
    "#         print('Multi-Scale Temporal Relation Network Module in use', ['%d-frame relation' % i for i in self.scales])\n",
    "\n",
    "    def forward(self, input):\n",
    "        # the first one is the largest scale - global features\n",
    "        act_all = input[:, self.relations_scales[0][0] , :]\n",
    "        act_all = act_all.view(act_all.size(0), self.scales[0] * self.img_feature_dim)\n",
    "        act_all = self.fc_fusion_scales[0](act_all)\n",
    "\n",
    "        for scaleID in range(1, len(self.scales)):\n",
    "            # iterate over the scales - local features, features from images at different scales\n",
    "            idx_relations_randomsample = np.random.choice(len(self.relations_scales[scaleID]), self.subsample_scales[scaleID], replace=False)\n",
    "            for idx in idx_relations_randomsample:\n",
    "                act_relation = input[:, self.relations_scales[scaleID][idx], :]\n",
    "                act_relation = act_relation.view(act_relation.size(0), self.scales[scaleID] * self.img_feature_dim)\n",
    "                act_relation = self.fc_fusion_scales[scaleID](act_relation)\n",
    "                act_all += act_relation\n",
    "        return act_all\n",
    "\n",
    "    def return_relationset(self, num_frames, num_frames_relation): # creates the random set of all relation sets (for 3 frames in this case)\n",
    "        import itertools\n",
    "        return list(itertools.combinations([i for i in range(num_frames)], num_frames_relation))\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "num_frames = 3\n",
    "num_classes = 4  # Replace with the actual number of classes\n",
    "input_size = (3, 224, 224)  # Replace with the actual input size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"E:/extracted datasets/Anomaly-Videos-Part-1\" #Root directory of the dataset\n",
    "# output_path = \"features/frames\" #Directory to save the extracted frames\n",
    "features_path = \"features/\" #Path to save the extracted features as tensors\n",
    "# labels_path = \"D:/dataset/labels.txt\" #Path to save the labels as a text file\n",
    "\n",
    "\n",
    "model = RelationModuleMultiScale(240*320, 3, 2048)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(240), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485,0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "labels = []\n",
    "    # Iterate over the videos in the folder\n",
    "for folder_name in [\"test\"]:\n",
    "    folder_path = os.path.join(dataset_path, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        continue\n",
    "    video_files = os.listdir(folder_path)\n",
    "\n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(folder_path, video_file)\n",
    "        features = []\n",
    "        # Open the video file\n",
    "        video_capture = cv2.VideoCapture(video_path)\n",
    "        frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Extract frames and features\n",
    "        for frame_index in range(0, frame_count):\n",
    "            success, frame = video_capture.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            # Extract features using ResNet50\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_tensor = transform(frame)\n",
    "            frame_tensor = torch.unsqueeze(frame_tensor, 0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feature_tensor = model(frame_tensor)\n",
    "\n",
    "            # Append features and labels\n",
    "            features.append(feature_tensor.squeeze().cpu().numpy())\n",
    "            labels.append(folder_name)\n",
    "\n",
    "\n",
    "        video_capture.release()\n",
    "\n",
    "        # Convert features to tensors\n",
    "        features_tensor = torch.tensor(features)\n",
    "        savnm = video_file+\".pt\"\n",
    "        \n",
    "        featurept = features_path+'/'+video_file+'.pt'\n",
    "       \n",
    "        torch.save(features_tensor, featurept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "amUilBIwX40d"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
