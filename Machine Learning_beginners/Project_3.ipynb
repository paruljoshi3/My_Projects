{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.1-py2.py3-none-any.whl (80 kB)\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from newspaper3k) (4.8.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from newspaper3k) (7.0.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from newspaper3k) (2.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from newspaper3k) (5.3)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from newspaper3k) (3.4.5)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from newspaper3k) (4.5.0)\n",
      "Collecting tldextract>=2.0.1\n",
      "  Downloading tldextract-2.2.3-py2.py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: six in c:\\users\\parul\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.14.0)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (1.9.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.25.8)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Building wheels for collected packages: feedfinder2, jieba3k, tinysegmenter, sgmllib3k\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3359 sha256=f3191d6fe89edec011853d6c19021198b0da32f08f8ec90fe4f6287d82812d5b\n",
      "  Stored in directory: c:\\users\\parul\\appdata\\local\\pip\\cache\\wheels\\7f\\d4\\8f\\6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398410 sha256=d51813329f9d583c9c18444c4c5b6a92fb61273307faea89d7c6d5ed950672f6\n",
      "  Stored in directory: c:\\users\\parul\\appdata\\local\\pip\\cache\\wheels\\4c\\91\\46\\3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13542 sha256=b10bb0d0c9b96fc60b3394b954cd33375ae9ab68289839a22e73c12914089281\n",
      "  Stored in directory: c:\\users\\parul\\appdata\\local\\pip\\cache\\wheels\\df\\67\\41\\faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6071 sha256=a06949d9406535dd595b1f38440d0c16be9f7376176659dc27e830d06a2f4727\n",
      "  Stored in directory: c:\\users\\parul\\appdata\\local\\pip\\cache\\wheels\\73\\ad\\a4\\0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
      "Successfully built feedfinder2 jieba3k tinysegmenter sgmllib3k\n",
      "Installing collected packages: feedfinder2, sgmllib3k, feedparser, jieba3k, tinysegmenter, requests-file, tldextract, newspaper3k\n",
      "Successfully installed feedfinder2-0.0.4 feedparser-6.0.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.2.0.tar.gz (113 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\parul\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parul\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-2.2.0-py3-none-any.whl size=116517 sha256=4cc0ded449d918c9e78e7c12c70bfe8677fe342e9a309987126144602d2bd0ff\n",
      "  Stored in directory: c:\\users\\parul\\appdata\\local\\pip\\cache\\wheels\\4e\\72\\e3\\c063e4bc1b764141ed22539c0482d1a4d62343e11a58c5bf88\n",
      "Successfully built smart-open\n",
      "Installing collected packages: Cython, smart-open, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.15\n",
      "    Uninstalling Cython-0.29.15:\n",
      "      Successfully uninstalled Cython-0.29.15\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-ldap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e2e36622ffaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamulticore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLdaMulticore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m \u001b[1;31m#LDA visualization library\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim #the library for Topic modelling\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim #LDA visualization library\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_xlsx(r'D:\\News_topics.xlsx)\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    stop_free = ' '.join([word for word in text.lower().split() if word not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = ' '.join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    return normalized.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean']=df['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(df['text_clean'])\n",
    "#Total number of non-zeroes in the BOW matrix (sum of the number of unique words per document over the entire corpus).\n",
    "print(dictionary.num_nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in df['text_clean'] ]\n",
    "print(len(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=3\n",
    "%time ldamodel = lda(doc_term_matrix,num_topics=num_topics,id2word=dictionary,passes=50,minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.print_topics(num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, sort_topics=False, mds='mmds')\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_corpus = ldamodel[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "\n",
    "threshold = sum(scores)/len(scores)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 = [j for i,j in zip(lda_corpus,df.index) if i[0][1] > threshold]\n",
    "cluster2 = [j for i,j in zip(lda_corpus,df.index) if i[1][1] > threshold]\n",
    "cluster3 = [j for i,j in zip(lda_corpus,df.index) if i[2][1] > threshold]\n",
    "# cluster4 = [j for i,j in zip(lda_corpus,df.index) if i[3][1] > threshold]\n",
    "# cluster5 = [j for i,j in zip(lda_corpus,df.index) if i[4][1] > threshold]\n",
    "\n",
    "print(len(cluster1))\n",
    "print(len(cluster2))\n",
    "print(len(cluster3))\n",
    "# print(len(cluster4))\n",
    "# print(len(cluster5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[cluster1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[cluster2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[cluster3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
